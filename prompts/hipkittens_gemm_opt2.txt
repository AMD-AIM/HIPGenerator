You are an expert HIP/C++ programmer generating high-performance GEMM kernels for AMD MI350X GPUs using HipKittens library.

## OPTIMIZATION LEVEL 2: Deeper Pipelining + Fine-grained Waitcnt

This builds on Level 1 (shared memory double buffering) and adds:
1. **Deeper pipelining** - Prefetch 2 tiles ahead
2. **Fine-grained waitcnt** - Don't always wait for 0, allow overlap
3. **Interleaved load/compute** - Better instruction scheduling

## KEY OPTIMIZATION: 2-Tile Prefetch

```cpp
// PROLOGUE: Load first 2 tiles before main loop
G::load(As[0][0], g_A, {0, 0, row*2, 0}, swizzled_A);
G::load(As[0][1], g_A, {0, 0, row*2+1, 0}, swizzled_A);
G::load(Bs[0][0], g_Bt, {0, 0, col*2, 0}, swizzled_B);
G::load(Bs[0][1], g_Bt, {0, 0, col*2+1, 0}, swizzled_B);
asm volatile("s_waitcnt vmcnt(0)\n");
__builtin_amdgcn_s_barrier();

G::load(As[1][0], g_A, {0, 0, row*2, 1}, swizzled_A);
G::load(As[1][1], g_A, {0, 0, row*2+1, 1}, swizzled_A);
G::load(Bs[1][0], g_Bt, {0, 0, col*2, 1}, swizzled_B);
G::load(Bs[1][1], g_Bt, {0, 0, col*2+1, 1}, swizzled_B);
asm volatile("s_waitcnt vmcnt(4)\n");  // Wait only for first 4 loads
__builtin_amdgcn_s_barrier();

// MAIN LOOP: Compute tile k, prefetch tile k+2
for (int k = 0; k < num_k_tiles; k++) {
    // Compute with tile[tic]
    // ...
    
    // Prefetch tile k+2 to buffer[toc]
    if (k + 2 < num_k_tiles) {
        G::load(As[toc][0], g_A, {0, 0, row*2, k+2}, swizzled_A);
        G::load(As[toc][1], g_A, {0, 0, row*2+1, k+2}, swizzled_A);
        G::load(Bs[toc][0], g_Bt, {0, 0, col*2, k+2}, swizzled_B);
        G::load(Bs[toc][1], g_Bt, {0, 0, col*2+1, k+2}, swizzled_B);
    }
    
    if (k + 2 < num_k_tiles) asm volatile("s_waitcnt vmcnt(4)\n");
    __builtin_amdgcn_s_barrier();
    tic ^= 1; toc ^= 1;
}
```

## COMPLETE KERNEL

```cpp
#include <torch/extension.h>
#include <hip/hip_runtime.h>
#include "kittens.cuh"
using namespace kittens;

constexpr int BLOCK_SIZE = 256;
constexpr int HALF_BLOCK_SIZE = 128;
constexpr int K_STEP = 64;
constexpr int WARPS_M = 2, WARPS_N = 4;
constexpr int HALF_REG_M = 64, HALF_REG_N = 32;
#define NUM_WARPS (WARPS_M * WARPS_N)
#define NUM_THREADS (NUM_WARPS * kittens::WARP_THREADS)

using _gl = gl<bf16, -1, -1, -1, -1>;
using ST_A = st_bf<HALF_BLOCK_SIZE, K_STEP, st_16x32_s>;
using ST_B = st_bf<HALF_BLOCK_SIZE, K_STEP, st_16x32_s>;
using G = kittens::group<NUM_WARPS>;

__global__ __launch_bounds__(NUM_THREADS, 2)
void gemm_kernel(const _gl g_A, const _gl g_Bt, _gl g_C, int M, int K, int N) {
    extern __shared__ alignment_dummy __shm[];
    shared_allocator al((int*)&__shm[0]);
    ST_A (&As)[2][2] = al.allocate<ST_A, 2, 2>();
    ST_B (&Bs)[2][2] = al.allocate<ST_B, 2, 2>();
    
    const int warp_id = kittens::warpid();
    const int warp_row = warp_id / WARPS_N;
    const int warp_col = warp_id % WARPS_N;
    const int row = blockIdx.x;
    const int col = blockIdx.y;
    
    rt_bf<HALF_REG_M, K_STEP, row_l, rt_16x32_s> a_reg;
    rt_bf<HALF_REG_N, K_STEP, row_l, rt_16x32_s> b_reg_0, b_reg_1;
    rt_fl<HALF_REG_M, HALF_REG_N, col_l, rt_16x16_s> c_accum[2][2];
    zero(c_accum[0][0]); zero(c_accum[0][1]);
    zero(c_accum[1][0]); zero(c_accum[1][1]);
    
    // Swizzled offsets
    using T = typename ST_A::dtype;
    constexpr int bytes_per_thread = ST_A::underlying_subtile_bytes_per_thread;
    constexpr int bytes_per_memcpy = bytes_per_thread * NUM_THREADS;
    constexpr int memcpy_per_tile = BLOCK_SIZE * K_STEP * sizeof(T) / bytes_per_memcpy;
    uint32_t swizzled_A[memcpy_per_tile/2];
    uint32_t swizzled_B[memcpy_per_tile/2];
    G::prefill_swizzled_offsets(As[0][0], g_A, swizzled_A);
    G::prefill_swizzled_offsets(Bs[0][0], g_Bt, swizzled_B);
    
    const int num_k_tiles = K / K_STEP;
    int tic = 0, toc = 1;
    
    // PROLOGUE: Load first tile
    G::load(As[0][0], g_A, {0, 0, row*2, 0}, swizzled_A);
    G::load(As[0][1], g_A, {0, 0, row*2+1, 0}, swizzled_A);
    G::load(Bs[0][0], g_Bt, {0, 0, col*2, 0}, swizzled_B);
    G::load(Bs[0][1], g_Bt, {0, 0, col*2+1, 0}, swizzled_B);
    asm volatile("s_waitcnt vmcnt(0)\n");
    __builtin_amdgcn_s_barrier();
    
    // Load second tile (for deeper pipeline)
    if (num_k_tiles > 1) {
        G::load(As[1][0], g_A, {0, 0, row*2, 1}, swizzled_A);
        G::load(As[1][1], g_A, {0, 0, row*2+1, 1}, swizzled_A);
        G::load(Bs[1][0], g_Bt, {0, 0, col*2, 1}, swizzled_B);
        G::load(Bs[1][1], g_Bt, {0, 0, col*2+1, 1}, swizzled_B);
    }
    asm volatile("s_waitcnt vmcnt(4)\n");
    __builtin_amdgcn_s_barrier();
    
    // MAIN LOOP
    for (int k = 0; k < num_k_tiles; k++) {
        // Load subtiles from shared memory
        auto a_sub = subtile_inplace<HALF_REG_M, K_STEP>(As[tic][0], {warp_row, 0});
        load(a_reg, a_sub);
        auto b_sub0 = subtile_inplace<HALF_REG_N, K_STEP>(Bs[tic][0], {warp_col, 0});
        load(b_reg_0, b_sub0);
        auto b_sub1 = subtile_inplace<HALF_REG_N, K_STEP>(Bs[tic][1], {warp_col, 0});
        load(b_reg_1, b_sub1);
        
        // Prefetch k+2 tile
        if (k + 2 < num_k_tiles) {
            G::load(As[toc][0], g_A, {0, 0, row*2, k+2}, swizzled_A);
            G::load(As[toc][1], g_A, {0, 0, row*2+1, k+2}, swizzled_A);
            G::load(Bs[toc][0], g_Bt, {0, 0, col*2, k+2}, swizzled_B);
            G::load(Bs[toc][1], g_Bt, {0, 0, col*2+1, k+2}, swizzled_B);
        }
        
        asm volatile("s_waitcnt lgkmcnt(0)\n");
        __builtin_amdgcn_sched_barrier(0);
        __builtin_amdgcn_s_setprio(1);
        mma_ABt(c_accum[0][0], a_reg, b_reg_0, c_accum[0][0]);
        mma_ABt(c_accum[0][1], a_reg, b_reg_1, c_accum[0][1]);
        __builtin_amdgcn_s_setprio(0);
        __builtin_amdgcn_sched_barrier(0);
        
        a_sub = subtile_inplace<HALF_REG_M, K_STEP>(As[tic][1], {warp_row, 0});
        load(a_reg, a_sub);
        asm volatile("s_waitcnt lgkmcnt(0)\n");
        
        __builtin_amdgcn_sched_barrier(0);
        __builtin_amdgcn_s_setprio(1);
        mma_ABt(c_accum[1][0], a_reg, b_reg_0, c_accum[1][0]);
        mma_ABt(c_accum[1][1], a_reg, b_reg_1, c_accum[1][1]);
        __builtin_amdgcn_s_setprio(0);
        __builtin_amdgcn_sched_barrier(0);
        
        // Wait for prefetch
        if (k + 2 < num_k_tiles) asm volatile("s_waitcnt vmcnt(4)\n");
        else asm volatile("s_waitcnt vmcnt(0)\n");
        __builtin_amdgcn_s_barrier();
        tic ^= 1; toc ^= 1;
    }
    
    store(g_C, c_accum[0][0], {0, 0, row*2*WARPS_M + warp_row, col*2*WARPS_N + warp_col});
    store(g_C, c_accum[0][1], {0, 0, row*2*WARPS_M + warp_row, col*2*WARPS_N + WARPS_N + warp_col});
    store(g_C, c_accum[1][0], {0, 0, row*2*WARPS_M + WARPS_M + warp_row, col*2*WARPS_N + warp_col});
    store(g_C, c_accum[1][1], {0, 0, row*2*WARPS_M + WARPS_M + warp_row, col*2*WARPS_N + WARPS_N + warp_col});
}

torch::Tensor gemm_forward(torch::Tensor A, torch::Tensor Bt, int orig_M, int orig_N) {
    const int M = A.size(0), K = A.size(1), N = Bt.size(0);
    auto C = torch::zeros({M, N}, A.options());
    
    _gl g_A{(bf16*)A.data_ptr<at::BFloat16>(), 1u, 1u, (unsigned)M, (unsigned)K};
    _gl g_Bt{(bf16*)Bt.data_ptr<at::BFloat16>(), 1u, 1u, (unsigned)N, (unsigned)K};
    _gl g_C{(bf16*)C.data_ptr<at::BFloat16>(), 1u, 1u, (unsigned)M, (unsigned)N};
    
    dim3 grid(M / BLOCK_SIZE, N / BLOCK_SIZE);
    dim3 block(NUM_THREADS);
    size_t smem = MAX_SHARED_MEMORY;
    
    hipFuncSetAttribute((void*)gemm_kernel, hipFuncAttributeMaxDynamicSharedMemorySize, smem);
    gemm_kernel<<<grid, block, smem>>>(g_A, g_Bt, g_C, M, K, N);
    
    return C.slice(0, 0, orig_M).slice(1, 0, orig_N);
}
```

## REQUIRED PYTHON STRUCTURE
```python
cpp_src = r'''
#include <torch/extension.h>
torch::Tensor gemm_forward(torch::Tensor A, torch::Tensor Bt, int orig_M, int orig_N);
'''

hip_src = r'''
// Complete kernel above
'''

module = load_inline(
    name="hipkittens_gemm",
    cpp_sources=cpp_src,
    cuda_sources=hip_src,
    functions=["gemm_forward"],
    with_cuda=True,
    extra_cuda_cflags=[
        "-O3", "-std=c++20",
        "-I/root/agent/HipKittens/include",
        "-I/opt/rocm/include/hip",
        "-DKITTENS_CDNA4", "-DHIP_ENABLE_WARP_SYNC_BUILTINS",
        "--offload-arch=gfx950",
    ],
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self._Bt_cache = None
        self._B_id = None
    
    def forward(self, A, B):
        orig_M, orig_K, orig_N = A.size(0), A.size(1), B.size(1)
        BLOCK = 256
        M = ((orig_M + BLOCK - 1) // BLOCK) * BLOCK
        K_pad = ((orig_K + 63) // 64) * 64
        N = ((orig_N + BLOCK - 1) // BLOCK) * BLOCK
        
        if M != orig_M or K_pad != orig_K:
            A_pad = torch.zeros(M, K_pad, dtype=A.dtype, device=A.device)
            A_pad[:orig_M, :orig_K] = A
        else:
            A_pad = A
        
        if self._B_id != id(B) or self._Bt_cache is None:
            B_pad = torch.zeros(K_pad, N, dtype=B.dtype, device=B.device)
            B_pad[:orig_K, :orig_N] = B
            self._Bt_cache = B_pad.T.contiguous()
            self._B_id = id(B)
        
        return module.gemm_forward(A_pad, self._Bt_cache, orig_M, orig_N)
```

## OUTPUT
Output ONLY Python code in ```python ... ``` block.
NO explanations outside code block.
