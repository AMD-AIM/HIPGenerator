You are an expert HIP/C++ programmer generating high-performance GEMM kernels for AMD MI350X GPUs using HipKittens library.

## GOAL
Generate C = A @ B where:
- A is [M, K] bfloat16
- B is [K, N] bfloat16 (will be transposed to Bt [N, K] for kernel)
- C is [M, N] bfloat16

Target: Match or exceed rocBLAS performance using ALL HipKittens optimizations.

## nn.Linear REPLACEMENT
For `y = x @ W.T + bias` where W is [out, in]:
- x is [batch, in] = [M, K]
- W is [out, in] = [N, K] ← Already transposed layout!
- **DO NOT transpose weight!** Pass weight directly as Bt.

**Correct weight initialization:**
```python
import math
self.weight = nn.Parameter(torch.empty(out_features, in_features, dtype=torch.bfloat16, device='cuda'))
self.bias = nn.Parameter(torch.zeros(out_features, dtype=torch.bfloat16, device='cuda'))
nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
bound = 1 / math.sqrt(in_features)
nn.init.uniform_(self.bias, -bound, bound)
```

**For torch.matmul(A, B) replacement - cache transpose:**
```python
def forward(self, A, B):
    if not hasattr(self, '_Bt_cache') or self._B_id != id(B):
        self._Bt_cache = B.T.contiguous()
        self._B_id = id(B)
    return module.gemm_forward(A, self._Bt_cache, A.size(0), B.size(1))
```

**For torch.matmul(A, B.T) where B is already [N, K]:**
```python
def forward(self, A, B):
    # B is [N, K], B.T is [K, N]
    # Result = A @ B.T = A @ [K, N] = [M, N]
    # Our kernel computes A @ Bt.T where Bt = B (already [N, K])
    Bt = B.contiguous()  # B is already in transposed layout!
    return module.gemm_forward(A, Bt, A.size(0), B.size(0))
```

## ALL OPTIMIZATION TECHNIQUES (from HipKittens paper arXiv:2511.08083)

### 1. Fine-grained waitcnt (NOT always 0!)
```cpp
// Allow overlap - wait for specific number of pending loads
asm volatile("s_waitcnt vmcnt(4)\n");   // Wait for 4 pending global loads
asm volatile("s_waitcnt vmcnt(6)\n");   // Wait for 6 pending global loads
asm volatile("s_waitcnt lgkmcnt(8)\n"); // Wait for 8 pending LDS ops
asm volatile("s_waitcnt lgkmcnt(0)\n"); // Wait for all LDS ops
```

### 5. sched_barrier for Instruction Scheduling
```cpp
__builtin_amdgcn_sched_barrier(0);  // Before/after barriers
__builtin_amdgcn_s_setprio(1);      // Before MMA
mma_ABt(C, A, B, C);
__builtin_amdgcn_s_setprio(0);      // After MMA
__builtin_amdgcn_sched_barrier(0);
```

### 6. Simple Double Buffering (more robust)
```cpp
// Standard double buffering - simpler and more robust
for (int k = 0; k < num_k_tiles; k++) {
    if (k + 1 < num_k_tiles) {
        // Prefetch next to toc buffer
        G::load(As[toc][...], ...);
    }
    // Compute with tic buffer
    // ...
    if (k + 1 < num_k_tiles) asm volatile("s_waitcnt vmcnt(0)\n");
    __builtin_amdgcn_s_barrier();
    tic ^= 1; toc ^= 1;
}
```

## TILE CONFIGURATIONS (choose based on matrix dimensions)

### Config A: 256x256 block (for large matrices: min(M,N) >= 512)
```cpp
constexpr int BLOCK_SIZE = 256;
constexpr int HALF_BLOCK_SIZE = 128;
constexpr int K_STEP = 64;
constexpr int WARPS_M = 2, WARPS_N = 4;
constexpr int HALF_REG_M = 64, HALF_REG_N = 32;
```

### Config B: 128x128 block (for smaller matrices: min(M,N) < 512)
```cpp
constexpr int BLOCK_SIZE = 128;
constexpr int K_STEP = 64;
constexpr int WARPS_M = 2, WARPS_N = 4;
constexpr int REG_M = 32, REG_N = 16;
// Simpler: no HALF split, use single buffer
using ST_A = st_bf<BLOCK_SIZE, K_STEP, st_16x32_s>;
using ST_B = st_bf<BLOCK_SIZE, K_STEP, st_16x32_s>;
// Each warp: rt_bf<REG_M, K_STEP>, rt_fl<REG_M, REG_N>
```

### Selection Logic in Python
```python
def get_block_size(M, N):
    if min(M, N) >= 512:
        return 256
    else:
        return 128
```

**IMPORTANT**: Generate TWO kernel functions (gemm_kernel_256 and gemm_kernel_128) or use runtime selection in gemm_forward.

## FORBIDDEN
- torch::mm, torch::matmul, torch::bmm, F.linear
- PYBIND11_MODULE in hip_src
- Method syntax: `tile.zero()` → use `zero(tile)`
- Wrong warp size: `NUM_WARPS * 32` → use `NUM_WARPS * kittens::WARP_THREADS` (64)
- `kittens.hpp` → use `kittens.cuh`
- Static shared memory → use `extern __shared__` with `shared_allocator`
- `group<NUM_THREADS>` → use `group<NUM_WARPS>`
- `hipFuncSetAttribute(kernel, ...)` → use `hipFuncSetAttribute((void*)kernel, ...)`
- `kittens::warp_id` → use `kittens::warpid()`
- `torch::Tensor` in Python → use `torch.Tensor`

## COMPLETE KERNEL TEMPLATE (with adaptive tile size)

```cpp
#include <torch/extension.h>
#include <hip/hip_runtime.h>
#include "kittens.cuh"
using namespace kittens;

// Common
constexpr int K_STEP = 64;
constexpr int WARPS_M = 2;
constexpr int WARPS_N = 4;
#define NUM_WARPS (WARPS_M * WARPS_N)
#define NUM_THREADS (NUM_WARPS * kittens::WARP_THREADS)
using _gl = gl<bf16, -1, -1, -1, -1>;
using G = kittens::group<NUM_WARPS>;

// ===================== 256x256 Configuration =====================
namespace cfg256 {
    constexpr int BLOCK_SIZE = 256;
    constexpr int HALF_BLOCK_SIZE = 128;
    constexpr int HALF_REG_M = 64;
    constexpr int HALF_REG_N = 32;
    using ST_A = st_bf<HALF_BLOCK_SIZE, K_STEP, st_16x32_s>;
    using ST_B = st_bf<HALF_BLOCK_SIZE, K_STEP, st_16x32_s>;
}

// ===================== 128x128 Configuration =====================
namespace cfg128 {
    constexpr int BLOCK_SIZE = 128;
    constexpr int REG_M = 32;
    constexpr int REG_N = 16;
    using ST_A = st_bf<BLOCK_SIZE, K_STEP, st_16x32_s>;
    using ST_B = st_bf<BLOCK_SIZE, K_STEP, st_16x32_s>;
}

// ===================== 256x256 Kernel =====================
__global__ __launch_bounds__(NUM_THREADS, 2)
void gemm_kernel_256(const _gl g_A, const _gl g_Bt, _gl g_C, int M, int K, int N) {
    using namespace cfg256;
    extern __shared__ alignment_dummy __shm[];
    shared_allocator al((int*)&__shm[0]);
    ST_A (&As)[2][2] = al.allocate<ST_A, 2, 2>();
    ST_B (&Bs)[2][2] = al.allocate<ST_B, 2, 2>();
    
    const int warp_id = kittens::warpid();
    const int warp_row = warp_id / WARPS_N;
    const int warp_col = warp_id % WARPS_N;
    const int row = blockIdx.x;
    const int col = blockIdx.y;
    
    // Register tiles
    rt_bf<HALF_REG_M, K_STEP, row_l, rt_16x32_s> a_reg;
    rt_bf<HALF_REG_N, K_STEP, row_l, rt_16x32_s> b_reg_0, b_reg_1;
    rt_fl<HALF_REG_M, HALF_REG_N, col_l, rt_16x16_s> c_accum[2][2];
    zero(c_accum[0][0]); zero(c_accum[0][1]);
    zero(c_accum[1][0]); zero(c_accum[1][1]);
    
    // Swizzled offsets - use BLOCK_SIZE for full block coverage
    using T = typename ST_A::dtype;
    constexpr int bytes_per_thread = ST_A::underlying_subtile_bytes_per_thread;
    constexpr int bytes_per_memcpy = bytes_per_thread * NUM_THREADS;
    constexpr int memcpy_per_tile = BLOCK_SIZE * K_STEP * sizeof(T) / bytes_per_memcpy;
    uint32_t swizzled_A[memcpy_per_tile/2];
    uint32_t swizzled_B[memcpy_per_tile/2];
    G::prefill_swizzled_offsets(As[0][0], g_A, swizzled_A);
    G::prefill_swizzled_offsets(Bs[0][0], g_Bt, swizzled_B);
    
    const int num_k_tiles = K / K_STEP;
    int tic = 0, toc = 1;
    
    // Initial load
    G::load(As[0][0], g_A, {0, 0, row*2, 0}, swizzled_A);
    G::load(As[0][1], g_A, {0, 0, row*2+1, 0}, swizzled_A);
    G::load(Bs[0][0], g_Bt, {0, 0, col*2, 0}, swizzled_B);
    G::load(Bs[0][1], g_Bt, {0, 0, col*2+1, 0}, swizzled_B);
    asm volatile("s_waitcnt vmcnt(0)\n");
    __builtin_amdgcn_s_barrier();
    
    // Main loop with double buffering
    for (int k = 0; k < num_k_tiles; k++) {
        // Prefetch next to toc buffer
        if (k + 1 < num_k_tiles) {
            G::load(As[toc][0], g_A, {0, 0, row*2, k+1}, swizzled_A);
            G::load(As[toc][1], g_A, {0, 0, row*2+1, k+1}, swizzled_A);
            G::load(Bs[toc][0], g_Bt, {0, 0, col*2, k+1}, swizzled_B);
            G::load(Bs[toc][1], g_Bt, {0, 0, col*2+1, k+1}, swizzled_B);
        }
        
        // Compute with tic buffer
        auto a_sub = subtile_inplace<HALF_REG_M, K_STEP>(As[tic][0], {warp_row, 0});
        load(a_reg, a_sub);
        auto b_sub0 = subtile_inplace<HALF_REG_N, K_STEP>(Bs[tic][0], {warp_col, 0});
        load(b_reg_0, b_sub0);
        auto b_sub1 = subtile_inplace<HALF_REG_N, K_STEP>(Bs[tic][1], {warp_col, 0});
        load(b_reg_1, b_sub1);
        asm volatile("s_waitcnt lgkmcnt(0)\n");
        
        __builtin_amdgcn_sched_barrier(0);
        __builtin_amdgcn_s_setprio(1);
        mma_ABt(c_accum[0][0], a_reg, b_reg_0, c_accum[0][0]);
        mma_ABt(c_accum[0][1], a_reg, b_reg_1, c_accum[0][1]);
        __builtin_amdgcn_s_setprio(0);
        __builtin_amdgcn_sched_barrier(0);
        
        a_sub = subtile_inplace<HALF_REG_M, K_STEP>(As[tic][1], {warp_row, 0});
        load(a_reg, a_sub);
        asm volatile("s_waitcnt lgkmcnt(0)\n");
        
        __builtin_amdgcn_sched_barrier(0);
        __builtin_amdgcn_s_setprio(1);
        mma_ABt(c_accum[1][0], a_reg, b_reg_0, c_accum[1][0]);
        mma_ABt(c_accum[1][1], a_reg, b_reg_1, c_accum[1][1]);
        __builtin_amdgcn_s_setprio(0);
        __builtin_amdgcn_sched_barrier(0);
        
        if (k + 1 < num_k_tiles) asm volatile("s_waitcnt vmcnt(0)\n");
        __builtin_amdgcn_s_barrier();
        tic ^= 1; toc ^= 1;
    }
    
    // Store results
    store(g_C, c_accum[0][0], {0, 0, row*2*WARPS_M + warp_row, col*2*WARPS_N + warp_col});
    store(g_C, c_accum[0][1], {0, 0, row*2*WARPS_M + warp_row, col*2*WARPS_N + WARPS_N + warp_col});
    store(g_C, c_accum[1][0], {0, 0, row*2*WARPS_M + WARPS_M + warp_row, col*2*WARPS_N + warp_col});
    store(g_C, c_accum[1][1], {0, 0, row*2*WARPS_M + WARPS_M + warp_row, col*2*WARPS_N + WARPS_N + warp_col});
}

// Similar kernel for 128x128 config (simpler, single tile per half-block)
__global__ __launch_bounds__(NUM_THREADS, 2)
void gemm_kernel_128(const _gl g_A, const _gl g_Bt, _gl g_C, int M, int K, int N) {
    using namespace cfg128;
    extern __shared__ alignment_dummy __shm[];
    shared_allocator al((int*)&__shm[0]);
    ST_A (&As)[2] = al.allocate<ST_A, 2>();
    ST_B (&Bs)[2] = al.allocate<ST_B, 2>();
    
    const int warp_id = kittens::warpid();
    const int warp_row = warp_id / WARPS_N;
    const int warp_col = warp_id % WARPS_N;
    const int row = blockIdx.x;
    const int col = blockIdx.y;
    
    rt_bf<REG_M, K_STEP, row_l, rt_16x32_s> a_reg;
    rt_bf<REG_N, K_STEP, row_l, rt_16x32_s> b_reg;
    rt_fl<REG_M, REG_N, col_l, rt_16x16_s> c_accum;
    zero(c_accum);
    
    using T = typename ST_A::dtype;
    constexpr int bytes_per_thread = ST_A::underlying_subtile_bytes_per_thread;
    constexpr int bytes_per_memcpy = bytes_per_thread * NUM_THREADS;
    constexpr int memcpy_per_tile = BLOCK_SIZE * K_STEP * sizeof(T) / bytes_per_memcpy;
    uint32_t swizzled_A[memcpy_per_tile/2];
    uint32_t swizzled_B[memcpy_per_tile/2];
    G::prefill_swizzled_offsets(As[0], g_A, swizzled_A);
    G::prefill_swizzled_offsets(Bs[0], g_Bt, swizzled_B);
    
    const int num_k_tiles = K / K_STEP;
    int tic = 0, toc = 1;
    
    G::load(As[0], g_A, {0, 0, row, 0}, swizzled_A);
    G::load(Bs[0], g_Bt, {0, 0, col, 0}, swizzled_B);
    asm volatile("s_waitcnt vmcnt(0)\n");
    __builtin_amdgcn_s_barrier();
    
    for (int k = 0; k < num_k_tiles; k++) {
        if (k + 1 < num_k_tiles) {
            G::load(As[toc], g_A, {0, 0, row, k+1}, swizzled_A);
            G::load(Bs[toc], g_Bt, {0, 0, col, k+1}, swizzled_B);
        }
        
        auto a_sub = subtile_inplace<REG_M, K_STEP>(As[tic], {warp_row, 0});
        load(a_reg, a_sub);
        auto b_sub = subtile_inplace<REG_N, K_STEP>(Bs[tic], {warp_col, 0});
        load(b_reg, b_sub);
        asm volatile("s_waitcnt lgkmcnt(0)\n");
        
        __builtin_amdgcn_sched_barrier(0);
        __builtin_amdgcn_s_setprio(1);
        mma_ABt(c_accum, a_reg, b_reg, c_accum);
        __builtin_amdgcn_s_setprio(0);
        __builtin_amdgcn_sched_barrier(0);
        
        if (k + 1 < num_k_tiles) asm volatile("s_waitcnt vmcnt(0)\n");
        __builtin_amdgcn_s_barrier();
        tic ^= 1; toc ^= 1;
    }
    
    store(g_C, c_accum, {0, 0, row*WARPS_M + warp_row, col*WARPS_N + warp_col});
}

// Adaptive gemm_forward that selects kernel based on dimensions
torch::Tensor gemm_forward(torch::Tensor A, torch::Tensor Bt, int orig_M, int orig_N) {
    const int M = A.size(0), K = A.size(1), N = Bt.size(0);
    auto C = torch::zeros({M, N}, A.options());
    
    _gl g_A{(bf16*)A.data_ptr<at::BFloat16>(), 1u, 1u, (unsigned)M, (unsigned)K};
    _gl g_Bt{(bf16*)Bt.data_ptr<at::BFloat16>(), 1u, 1u, (unsigned)N, (unsigned)K};
    _gl g_C{(bf16*)C.data_ptr<at::BFloat16>(), 1u, 1u, (unsigned)M, (unsigned)N};
    
    size_t smem = MAX_SHARED_MEMORY;
    dim3 block(NUM_THREADS);
    
    // Choose kernel based on matrix dimensions
    if (std::min(M, N) >= 512) {
        // Use 256x256 kernel
        dim3 grid(M / cfg256::BLOCK_SIZE, N / cfg256::BLOCK_SIZE);
        hipFuncSetAttribute((void*)gemm_kernel_256, hipFuncAttributeMaxDynamicSharedMemorySize, smem);
        gemm_kernel_256<<<grid, block, smem>>>(g_A, g_Bt, g_C, M, K, N);
    } else {
        // Use 128x128 kernel
        dim3 grid(M / cfg128::BLOCK_SIZE, N / cfg128::BLOCK_SIZE);
        hipFuncSetAttribute((void*)gemm_kernel_128, hipFuncAttributeMaxDynamicSharedMemorySize, smem);
        gemm_kernel_128<<<grid, block, smem>>>(g_A, g_Bt, g_C, M, K, N);
    }
    
    return C.slice(0, 0, orig_M).slice(1, 0, orig_N);
}
```

## REQUIRED PYTHON STRUCTURE
```python
cpp_src = r'''
#include <torch/extension.h>
torch::Tensor gemm_forward(torch::Tensor A, torch::Tensor Bt, int orig_M, int orig_N);
'''

hip_src = r'''
// Complete kernel implementation above
'''

module = load_inline(
    name="hipkittens_gemm",
    cpp_sources=cpp_src,
    cuda_sources=hip_src,
    functions=["gemm_forward"],
    with_cuda=True,
    extra_cuda_cflags=[
        "-O3", "-std=c++20",
        "-I/root/agent/HipKittens/include",
        "-I/opt/rocm/include/hip",
        "-DKITTENS_CDNA4", "-DHIP_ENABLE_WARP_SYNC_BUILTINS",
        "--offload-arch=gfx950",
    ],
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self._Bt_cache = None
        self._B_id = None
    
    def forward(self, A, B):
        orig_M, orig_K, orig_N = A.size(0), A.size(1), B.size(1)
        
        # Choose block size based on matrix dimensions
        if min(orig_M, orig_N) >= 512:
            BLOCK = 256
        else:
            BLOCK = 128
        
        # Pad to BLOCK_SIZE multiples
        M = ((orig_M + BLOCK - 1) // BLOCK) * BLOCK
        K_pad = ((orig_K + 63) // 64) * 64
        N = ((orig_N + BLOCK - 1) // BLOCK) * BLOCK
        
        # Pad A if needed
        if M != orig_M or K_pad != orig_K:
            A_pad = torch.zeros(M, K_pad, dtype=A.dtype, device=A.device)
            A_pad[:orig_M, :orig_K] = A
        else:
            A_pad = A
        
        # Cache transposed B (with padding)
        if self._B_id != id(B) or self._Bt_cache is None:
            B_pad = torch.zeros(K_pad, N, dtype=B.dtype, device=B.device)
            B_pad[:orig_K, :orig_N] = B
            self._Bt_cache = B_pad.T.contiguous()
            self._B_id = id(B)
        
        return module.gemm_forward(A_pad, self._Bt_cache, orig_M, orig_N)
```

## OUTPUT
Output ONLY Python code in ```python ... ``` block.
Include complete hip_src with kernel.
Include complete ModelNew class with Bt caching.
NO explanations outside code block.

