You are an expert HIP/C++ programmer generating high-performance GEMM kernels for AMD MI350X GPUs using HipKittens library.

## OPTIMIZATION LEVEL 3: 8c4p Producer-Consumer Pattern

This is the HIGHEST optimization level, using:
1. **4 Producer waves** - Dedicated to global→shared memory loads
2. **8 Consumer waves** - Dedicated to shared→register→MMA computation
3. **Fully asynchronous** overlap between producers and consumers

## 8c4p ARCHITECTURE

```
Total: 12 waves = 4 producers + 8 consumers
- Producer group: warp_group 0 (warps 0-3)
- Consumer group 0: warp_group 1 (warps 4-7) → M_block 0
- Consumer group 1: warp_group 2 (warps 8-11) → M_block 1

Output: 128x256 per workgroup (2 M-blocks x 4 N-blocks x 32x32 per warp)
```

## KEY CONCEPTS

### 1. Warp Group Separation
```cpp
#define NUM_PRODUCER_WORKERS 4
#define NUM_CONSUMER_WORKERS 8  // 2 M-blocks x 4 warps
#define NUM_THREADS ((NUM_PRODUCER_WORKERS + NUM_CONSUMER_WORKERS) * kittens::WARP_THREADS)

int warp_id = kittens::warpid();
int warp_group_id = warpgroupid();  // 0=producers, 1,2=consumers
bool is_producer = (warp_group_id == 0);
bool is_consumer = (warp_group_id > 0 && warp_group_id <= M_BLOCK);
int consumer_idx = is_consumer ? warp_group_id - 1 : 0;  // Which M-block
int local_warp_id = warp_id % 4;  // Which warp within group (0-3)
```

### 2. Producer Group (group<4>)
```cpp
using G = kittens::group<NUM_PRODUCER_WORKERS>;  // 4 warps for loads

if (is_producer) {
    #pragma unroll
    for (int m = 0; m < M_BLOCK; m++) {
        G::load(As[tic][m][0], g_A, {0, 0, row*2 + 2*m + 0, tile}, swizzled_A);
        G::load(As[tic][m][1], g_A, {0, 0, row*2 + 2*m + 1, tile}, swizzled_A);
    }
    #pragma unroll
    for (int n = 0; n < N_BLOCK; n++) {
        G::load(Bs[tic][n][0], g_Bt, {0, 0, col*2 + 2*n + 0, tile}, swizzled_B);
        G::load(Bs[tic][n][1], g_Bt, {0, 0, col*2 + 2*n + 1, tile}, swizzled_B);
    }
    __builtin_amdgcn_s_waitcnt(0);
}
```

### 3. Consumer Computation
```cpp
if (is_consumer) {
    // Each consumer warp handles one N-block column (local_warp_id = 0,1,2,3)
    // Each consumer group handles one M-block row (consumer_idx = 0 or 1)
    
    auto b_sub0 = subtile_inplace<HALF_BLOCK, K_STEP>(Bs[tic][local_warp_id][0], {0, 0});
    load(b_reg_0, b_sub0);
    auto a_sub = subtile_inplace<HALF_BLOCK, K_STEP>(As[tic][consumer_idx][0], {0, 0});
    load(a_reg, a_sub);
    
    asm volatile("s_waitcnt lgkmcnt(0)");
    __builtin_amdgcn_s_setprio(1);
    mma_ABt(c_accum[0][0], a_reg, b_reg_0, c_accum[0][0]);
    __builtin_amdgcn_s_setprio(0);
    
    // ... continue with other subtiles
}
```

### 4. Synchronization Pattern
```cpp
// Producers finish loads, consumers wait
__builtin_amdgcn_sched_barrier(0);
__builtin_amdgcn_s_barrier();  // Sync all 12 warps

// Swap buffers
tic ^= 1; toc ^= 1;
```

## COMPLETE 8c4p KERNEL

```cpp
#include <torch/extension.h>
#include <hip/hip_runtime.h>
#include "kittens.cuh"
using namespace kittens;

constexpr int BLOCK_SIZE = 64;
constexpr int HALF_BLOCK = 32;
constexpr int K_STEP = 64;
constexpr int M_BLOCK = 2;
constexpr int N_BLOCK = 4;

constexpr int OUTPUT_M = BLOCK_SIZE * M_BLOCK;  // 128
constexpr int OUTPUT_N = BLOCK_SIZE * N_BLOCK;  // 256

#define NUM_PRODUCER_WORKERS 4
#define NUM_CONSUMER_WORKERS (M_BLOCK * 4)  // 8
#define NUM_THREADS ((NUM_PRODUCER_WORKERS + NUM_CONSUMER_WORKERS) * kittens::WARP_THREADS)
#define NUM_PRODUCER_THREADS (NUM_PRODUCER_WORKERS * kittens::WARP_THREADS)

using _gl = gl<bf16, -1, -1, -1, -1>;
using ST_A = st_bf<HALF_BLOCK, K_STEP, st_16x32_s>;
using ST_B = st_bf<HALF_BLOCK, K_STEP, st_16x32_s>;
using G = kittens::group<NUM_PRODUCER_WORKERS>;

// warpgroupid: 4 warps per group
__device__ __forceinline__ int warpgroupid() { return threadIdx.x >> 8; }  // threadIdx.x / 256

__global__ __launch_bounds__(NUM_THREADS, 2)
void gemm_kernel_8c4p(const _gl g_A, const _gl g_Bt, _gl g_C, int M, int K, int N) {
    extern __shared__ alignment_dummy __shm[];
    shared_allocator al((int*)&__shm[0]);
    
    // Shared memory: [buffer][m_block or n_block][half]
    ST_A (&As)[2][M_BLOCK][2] = al.allocate<ST_A, 2, M_BLOCK, 2>();
    ST_B (&Bs)[2][N_BLOCK][2] = al.allocate<ST_B, 2, N_BLOCK, 2>();
    
    // Register accumulators for consumers
    rt_fl<HALF_BLOCK, HALF_BLOCK, col_l, rt_16x16_s> c_accum[2][2];
    
    // Simple block indexing (XCD scheduling removed for simplicity)
    int row = blockIdx.y * M_BLOCK;
    int col = blockIdx.x * N_BLOCK;
    
    // Warp identification
    int warp_id = kittens::warpid();
    int warp_group_id = warpgroupid();
    bool is_producer = (warp_group_id == 0);
    bool is_consumer = (warp_group_id > 0 && warp_group_id <= M_BLOCK);
    int consumer_idx = is_consumer ? warp_group_id - 1 : 0;
    int local_warp_id = warp_id % 4;
    
    // Swizzled offsets (producers only)
    using T = typename ST_A::dtype;
    constexpr int bytes_per_thread = ST_A::underlying_subtile_bytes_per_thread;
    constexpr int bytes_per_memcpy = bytes_per_thread * NUM_PRODUCER_THREADS;
    constexpr int memcpy_per_tile = BLOCK_SIZE * K_STEP * sizeof(T) / bytes_per_memcpy;
    uint32_t swizzled_A[memcpy_per_tile];
    uint32_t swizzled_B[memcpy_per_tile];
    if (is_producer) {
        G::prefill_swizzled_offsets(As[0][0][0], g_A, swizzled_A);
        G::prefill_swizzled_offsets(Bs[0][0][0], g_Bt, swizzled_B);
    }
    
    int tic = 0, toc = 1;
    const int num_tiles = K / K_STEP;
    
    // Initial load
    if (is_producer) {
        #pragma unroll
        for (int m = 0; m < M_BLOCK; m++) {
            G::load(As[tic][m][0], g_A, {0, 0, row*2 + 2*m + 0, 0}, swizzled_A);
            G::load(As[tic][m][1], g_A, {0, 0, row*2 + 2*m + 1, 0}, swizzled_A);
        }
        #pragma unroll
        for (int n = 0; n < N_BLOCK; n++) {
            G::load(Bs[tic][n][0], g_Bt, {0, 0, col*2 + 2*n + 0, 0}, swizzled_B);
            G::load(Bs[tic][n][1], g_Bt, {0, 0, col*2 + 2*n + 1, 0}, swizzled_B);
        }
        __builtin_amdgcn_s_waitcnt(0);
    }
    __syncthreads();
    
    if (is_consumer) {
        zero(c_accum[0][0]); zero(c_accum[0][1]);
        zero(c_accum[1][0]); zero(c_accum[1][1]);
    }
    
    // Main loop
    #pragma unroll 1
    for (int tile = 0; tile < num_tiles - 1; tile++, tic ^= 1, toc ^= 1) {
        
        if (is_producer) {
            // Load next tile
            #pragma unroll
            for (int m = 0; m < M_BLOCK; m++) {
                G::load(As[toc][m][0], g_A, {0, 0, row*2 + 2*m + 0, tile + 1}, swizzled_A);
                G::load(As[toc][m][1], g_A, {0, 0, row*2 + 2*m + 1, tile + 1}, swizzled_A);
            }
            #pragma unroll
            for (int n = 0; n < N_BLOCK; n++) {
                G::load(Bs[toc][n][0], g_Bt, {0, 0, col*2 + 2*n + 0, tile + 1}, swizzled_B);
                G::load(Bs[toc][n][1], g_Bt, {0, 0, col*2 + 2*n + 1, tile + 1}, swizzled_B);
            }
            __builtin_amdgcn_s_waitcnt(0);
            
        } else if (is_consumer) {
            // Compute with current tile
            rt_bf<HALF_BLOCK, K_STEP, row_l, rt_16x32_s> a_reg;
            rt_bf<HALF_BLOCK, K_STEP, row_l, rt_16x32_s> b_reg_0, b_reg_1;
            
            auto b_sub0 = subtile_inplace<HALF_BLOCK, K_STEP>(Bs[tic][local_warp_id][0], {0, 0});
            load(b_reg_0, b_sub0);
            auto a_sub = subtile_inplace<HALF_BLOCK, K_STEP>(As[tic][consumer_idx][0], {0, 0});
            load(a_reg, a_sub);
            asm volatile("s_waitcnt lgkmcnt(0)");
            
            __builtin_amdgcn_s_setprio(1);
            mma_ABt(c_accum[0][0], a_reg, b_reg_0, c_accum[0][0]);
            __builtin_amdgcn_s_setprio(0);
            
            auto b_sub1 = subtile_inplace<HALF_BLOCK, K_STEP>(Bs[tic][local_warp_id][1], {0, 0});
            load(b_reg_1, b_sub1);
            asm volatile("s_waitcnt lgkmcnt(0)");
            
            __builtin_amdgcn_s_setprio(1);
            mma_ABt(c_accum[0][1], a_reg, b_reg_1, c_accum[0][1]);
            __builtin_amdgcn_s_setprio(0);
            
            a_sub = subtile_inplace<HALF_BLOCK, K_STEP>(As[tic][consumer_idx][1], {0, 0});
            load(a_reg, a_sub);
            asm volatile("s_waitcnt lgkmcnt(0)");
            
            __builtin_amdgcn_s_setprio(1);
            mma_ABt(c_accum[1][0], a_reg, b_reg_0, c_accum[1][0]);
            mma_ABt(c_accum[1][1], a_reg, b_reg_1, c_accum[1][1]);
            __builtin_amdgcn_s_setprio(0);
        }
        
        __builtin_amdgcn_sched_barrier(0);
        __builtin_amdgcn_s_barrier();
    }
    
    // Last tile - consumers only
    if (is_consumer) {
        rt_bf<HALF_BLOCK, K_STEP, row_l, rt_16x32_s> a_reg;
        rt_bf<HALF_BLOCK, K_STEP, row_l, rt_16x32_s> b_reg_0, b_reg_1;
        
        auto b_sub0 = subtile_inplace<HALF_BLOCK, K_STEP>(Bs[tic][local_warp_id][0], {0, 0});
        load(b_reg_0, b_sub0);
        auto a_sub = subtile_inplace<HALF_BLOCK, K_STEP>(As[tic][consumer_idx][0], {0, 0});
        load(a_reg, a_sub);
        asm volatile("s_waitcnt lgkmcnt(0)");
        
        __builtin_amdgcn_s_setprio(1);
        mma_ABt(c_accum[0][0], a_reg, b_reg_0, c_accum[0][0]);
        __builtin_amdgcn_s_setprio(0);
        
        auto b_sub1 = subtile_inplace<HALF_BLOCK, K_STEP>(Bs[tic][local_warp_id][1], {0, 0});
        load(b_reg_1, b_sub1);
        asm volatile("s_waitcnt lgkmcnt(0)");
        
        __builtin_amdgcn_s_setprio(1);
        mma_ABt(c_accum[0][1], a_reg, b_reg_1, c_accum[0][1]);
        __builtin_amdgcn_s_setprio(0);
        
        a_sub = subtile_inplace<HALF_BLOCK, K_STEP>(As[tic][consumer_idx][1], {0, 0});
        load(a_reg, a_sub);
        asm volatile("s_waitcnt lgkmcnt(0)");
        
        __builtin_amdgcn_s_setprio(1);
        mma_ABt(c_accum[1][0], a_reg, b_reg_0, c_accum[1][0]);
        mma_ABt(c_accum[1][1], a_reg, b_reg_1, c_accum[1][1]);
        __builtin_amdgcn_s_setprio(0);
    }
    
    // Store results (consumers only)
    if (is_consumer) {
        store(g_C, c_accum[0][0], {0, 0, (row + consumer_idx) * 2 + 0, (col + local_warp_id) * 2 + 0});
        store(g_C, c_accum[0][1], {0, 0, (row + consumer_idx) * 2 + 0, (col + local_warp_id) * 2 + 1});
        store(g_C, c_accum[1][0], {0, 0, (row + consumer_idx) * 2 + 1, (col + local_warp_id) * 2 + 0});
        store(g_C, c_accum[1][1], {0, 0, (row + consumer_idx) * 2 + 1, (col + local_warp_id) * 2 + 1});
    }
}

torch::Tensor gemm_forward(torch::Tensor A, torch::Tensor Bt, int orig_M, int orig_N) {
    const int M = A.size(0), K = A.size(1), N = Bt.size(0);
    auto C = torch::zeros({M, N}, A.options());
    
    _gl g_A{(bf16*)A.data_ptr<at::BFloat16>(), 1u, 1u, (unsigned)M, (unsigned)K};
    _gl g_Bt{(bf16*)Bt.data_ptr<at::BFloat16>(), 1u, 1u, (unsigned)N, (unsigned)K};
    _gl g_C{(bf16*)C.data_ptr<at::BFloat16>(), 1u, 1u, (unsigned)M, (unsigned)N};
    
    dim3 grid(N / OUTPUT_N, M / OUTPUT_M);
    dim3 block(NUM_THREADS);
    size_t smem = 98304;  // 96KB for producer-consumer
    
    hipFuncSetAttribute((void*)gemm_kernel_8c4p, hipFuncAttributeMaxDynamicSharedMemorySize, smem);
    gemm_kernel_8c4p<<<grid, block, smem>>>(g_A, g_Bt, g_C, M, K, N);
    
    return C.slice(0, 0, orig_M).slice(1, 0, orig_N);
}
```

## REQUIRED PYTHON STRUCTURE
```python
cpp_src = r'''
#include <torch/extension.h>
torch::Tensor gemm_forward(torch::Tensor A, torch::Tensor Bt, int orig_M, int orig_N);
'''

hip_src = r'''
// Complete 8c4p kernel implementation above
'''

module = load_inline(
    name="hipkittens_gemm_8c4p",
    cpp_sources=cpp_src,
    cuda_sources=hip_src,
    functions=["gemm_forward"],
    with_cuda=True,
    extra_cuda_cflags=[
        "-O3", "-std=c++20",
        "-I/root/agent/HipKittens/include",
        "-I/opt/rocm/include/hip",
        "-DKITTENS_CDNA4", "-DHIP_ENABLE_WARP_SYNC_BUILTINS",
        "--offload-arch=gfx950",
    ],
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self._Bt_cache = None
        self._B_id = None
    
    def forward(self, A, B):
        orig_M, orig_K, orig_N = A.size(0), A.size(1), B.size(1)
        
        # Pad to output block size (128x256)
        OUTPUT_M, OUTPUT_N = 128, 256
        K_STEP = 64
        M = ((orig_M + OUTPUT_M - 1) // OUTPUT_M) * OUTPUT_M
        K_pad = ((orig_K + K_STEP - 1) // K_STEP) * K_STEP
        N = ((orig_N + OUTPUT_N - 1) // OUTPUT_N) * OUTPUT_N
        
        if M != orig_M or K_pad != orig_K:
            A_pad = torch.zeros(M, K_pad, dtype=A.dtype, device=A.device)
            A_pad[:orig_M, :orig_K] = A
        else:
            A_pad = A
        
        if self._B_id != id(B) or self._Bt_cache is None:
            B_pad = torch.zeros(K_pad, N, dtype=B.dtype, device=B.device)
            B_pad[:orig_K, :orig_N] = B
            self._Bt_cache = B_pad.T.contiguous()
            self._B_id = id(B)
        
        return module.gemm_forward(A_pad, self._Bt_cache, orig_M, orig_N)
```

## CRITICAL NOTES
1. **12 warps total**: 4 producers (group<4>) + 8 consumers
2. **warpgroupid()**: Returns 0 for producers, 1-2 for consumer groups
3. **Producer-consumer separation**: if (is_producer) {...} else if (is_consumer) {...}
4. **Output size**: 128x256 per workgroup (M_BLOCK=2, N_BLOCK=4, BLOCK_SIZE=64)
5. **Grid calculation**: (N/256, M/128) - note order!

## OUTPUT
Output ONLY Python code in ```python ... ``` block.
Include complete hip_src with 8c4p kernel.
Include complete ModelNew class.
NO explanations outside code block.

