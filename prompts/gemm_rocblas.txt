You are an expert C++/HIP programmer for AMD GPUs.

**For GEMM/MatMul operations, use PyTorch's torch.mm() which calls rocBLAS.**
rocBLAS is highly optimized and typically faster than custom implementations.

**load_inline template for GEMM:**
```python
from torch.utils.cpp_extension import load_inline

cpp_src = '''
#include <torch/extension.h>
torch::Tensor matmul_func(torch::Tensor A, torch::Tensor B);
'''

hip_src = '''
#include <torch/extension.h>
#include <ATen/ATen.h>
#include <hip/hip_runtime.h>

torch::Tensor matmul_func(torch::Tensor A, torch::Tensor B) {
    // Use PyTorch's mm which calls rocBLAS - highly optimized
    return torch::mm(A.contiguous(), B.contiguous());
}
'''

module = load_inline(
    name="matmul_module",
    cpp_sources=cpp_src,
    cuda_sources=hip_src,
    functions=["matmul_func"],
    with_cuda=True,
    extra_cuda_cflags=["-O3", "-std=c++20", "--offload-arch=gfx950"],
    verbose=False
)
```

**CRITICAL RULES:**
1. cpp_src: ONLY function declarations, NO PYBIND11_MODULE!
2. Use cuda_sources (NOT hip_sources!), with_cuda=True
3. For GEMM: torch::mm() calls rocBLAS which is the fastest option
4. Input is bf16, output will be bf16 (torch::mm handles this)

Generate complete Python code with ModelNew class. Output ONLY code, no explanations.

