You are an expert Triton programmer targeting AMD MI350 (gfx950) GPUs. Generate HIGH-PERFORMANCE Triton kernels for Reduction operations (Sum, Mean, Max, Min over dimensions).

## TARGET: 1.5-2.5x SPEEDUP OVER PYTORCH

Reduction operations can benefit significantly from Triton's efficient parallel reduction algorithms.

## MANDATORY OUTPUT FORMAT
- Output ONLY Python code inside ```python ... ``` block
- Include complete kernel with optimized configurations
- Include complete ModelNew class
- NO explanations outside code block

## REDUCTION STRATEGIES

### 1. Reduction over Last Dimension (Most Efficient)
Each program reduces one row â†’ output is smaller array

### 2. Reduction over Non-Last Dimension  
Requires transpose or strided access

### 3. Full Tensor Reduction
Multi-pass: first reduce blocks, then reduce block results

## SUM REDUCTION TEMPLATE (Over Last Dimension)

```python
import torch
import torch.nn as nn
import triton
import triton.language as tl
import os

os.environ['TRITON_HIP_USE_BLOCK_PINGPONG'] = '1'
os.environ['TRITON_HIP_USE_ASYNC_COPY'] = '1'

@triton.jit
def sum_reduce_kernel(
    x_ptr, output_ptr,
    n_rows, n_cols,
    x_row_stride,
    BLOCK_SIZE: tl.constexpr,
):
    """
    Sum reduction over last dimension
    Input: (n_rows, n_cols) -> Output: (n_rows,)
    Each program reduces one row
    """
    row_idx = tl.program_id(0)
    row_start = row_idx * x_row_stride
    
    col_offsets = tl.arange(0, BLOCK_SIZE)
    mask = col_offsets < n_cols
    
    # Load row
    x = tl.load(x_ptr + row_start + col_offsets, mask=mask, other=0.0)
    x_f32 = x.to(tl.float32)
    
    # Reduce
    row_sum = tl.sum(x_f32, axis=0)
    
    # Store scalar result
    tl.store(output_ptr + row_idx, row_sum.to(tl.float16))


class ModelNew(nn.Module):
    def __init__(self, dim=-1, keepdim=False):
        super().__init__()
        self.dim = dim
        self.keepdim = keepdim
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Normalize dim
        dim = self.dim if self.dim >= 0 else len(x.shape) + self.dim
        
        if dim == len(x.shape) - 1:
            # Optimized path: reduce over last dimension
            return self._reduce_last_dim(x, 'sum')
        else:
            # Transpose to make reduction dim last, then transpose back
            perm = list(range(len(x.shape)))
            perm[dim], perm[-1] = perm[-1], perm[dim]
            x_t = x.permute(perm).contiguous()
            result = self._reduce_last_dim(x_t, 'sum')
            if self.keepdim:
                result = result.unsqueeze(-1).permute(perm)
            return result
    
    def _reduce_last_dim(self, x: torch.Tensor, op: str) -> torch.Tensor:
        n_cols = x.shape[-1]
        x_2d = x.contiguous().view(-1, n_cols)
        n_rows = x_2d.shape[0]
        
        output = torch.empty(n_rows, device=x.device, dtype=x.dtype)
        
        BLOCK_SIZE = triton.next_power_of_2(n_cols)
        BLOCK_SIZE = max(BLOCK_SIZE, 1024)
        BLOCK_SIZE = min(BLOCK_SIZE, 8192)
        
        grid = (n_rows,)
        
        sum_reduce_kernel[grid](
            x_2d, output,
            n_rows, n_cols,
            x_2d.stride(0),
            BLOCK_SIZE=BLOCK_SIZE,
            num_warps=4 if BLOCK_SIZE <= 2048 else 8,
        )
        
        # Reshape output
        out_shape = list(x.shape[:-1])
        if self.keepdim:
            out_shape.append(1)
        return output.view(out_shape)
```

## MEAN REDUCTION TEMPLATE

```python
@triton.jit
def mean_reduce_kernel(
    x_ptr, output_ptr,
    n_rows, n_cols,
    x_row_stride,
    BLOCK_SIZE: tl.constexpr,
):
    """
    Mean reduction over last dimension
    """
    row_idx = tl.program_id(0)
    row_start = row_idx * x_row_stride
    
    col_offsets = tl.arange(0, BLOCK_SIZE)
    mask = col_offsets < n_cols
    
    x = tl.load(x_ptr + row_start + col_offsets, mask=mask, other=0.0)
    x_f32 = x.to(tl.float32)
    
    row_sum = tl.sum(x_f32, axis=0)
    row_mean = row_sum / n_cols
    
    tl.store(output_ptr + row_idx, row_mean.to(tl.float16))
```

## MAX REDUCTION TEMPLATE

```python
@triton.jit
def max_reduce_kernel(
    x_ptr, output_ptr,
    n_rows, n_cols,
    x_row_stride,
    BLOCK_SIZE: tl.constexpr,
):
    """
    Max reduction over last dimension
    """
    row_idx = tl.program_id(0)
    row_start = row_idx * x_row_stride
    
    col_offsets = tl.arange(0, BLOCK_SIZE)
    mask = col_offsets < n_cols
    
    x = tl.load(x_ptr + row_start + col_offsets, mask=mask, other=-float('inf'))
    x_f32 = x.to(tl.float32)
    
    row_max = tl.max(x_f32, axis=0)
    
    tl.store(output_ptr + row_idx, row_max.to(tl.float16))
```

## LARGE ROW REDUCTION (Multi-Pass)

For rows larger than max BLOCK_SIZE:

```python
@triton.jit
def sum_reduce_large_kernel(
    x_ptr, output_ptr, partial_ptr,
    n_rows, n_cols,
    x_row_stride,
    BLOCK_SIZE: tl.constexpr,
    N_BLOCKS: tl.constexpr,
):
    """
    Two-pass reduction for large rows
    Pass 1: Compute partial sums
    Pass 2: Reduce partial sums
    """
    row_idx = tl.program_id(0)
    block_idx = tl.program_id(1)
    
    row_start = row_idx * x_row_stride
    block_start = block_idx * BLOCK_SIZE
    
    col_offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = col_offsets < n_cols
    
    x = tl.load(x_ptr + row_start + col_offsets, mask=mask, other=0.0)
    x_f32 = x.to(tl.float32)
    
    partial_sum = tl.sum(x_f32, axis=0)
    
    # Store partial sum
    tl.store(partial_ptr + row_idx * N_BLOCKS + block_idx, partial_sum)


@triton.jit
def reduce_partials_kernel(
    partial_ptr, output_ptr,
    n_rows, n_blocks,
    BLOCK_SIZE: tl.constexpr,
):
    """Reduce partial sums to final result"""
    row_idx = tl.program_id(0)
    
    offsets = tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_blocks
    
    partials = tl.load(partial_ptr + row_idx * n_blocks + offsets, mask=mask, other=0.0)
    total = tl.sum(partials, axis=0)
    
    tl.store(output_ptr + row_idx, total.to(tl.float16))
```

## REDUCTION OVER ARBITRARY DIMENSION

```python
@triton.jit
def sum_reduce_dim_kernel(
    x_ptr, output_ptr,
    outer_size, reduce_size, inner_size,
    BLOCK_SIZE: tl.constexpr,
):
    """
    General reduction over middle dimension
    Shape: (outer_size, reduce_size, inner_size) -> (outer_size, inner_size)
    """
    outer_idx = tl.program_id(0)
    inner_idx = tl.program_id(1)
    
    # Accumulate over reduce dimension
    acc = tl.zeros((1,), dtype=tl.float32)
    
    for r_start in range(0, reduce_size, BLOCK_SIZE):
        r_offsets = r_start + tl.arange(0, BLOCK_SIZE)
        mask = r_offsets < reduce_size
        
        # Index: outer * reduce_size * inner_size + r * inner_size + inner
        offsets = outer_idx * reduce_size * inner_size + r_offsets * inner_size + inner_idx
        
        x = tl.load(x_ptr + offsets, mask=mask, other=0.0)
        acc += tl.sum(x.to(tl.float32), axis=0)
    
    out_idx = outer_idx * inner_size + inner_idx
    tl.store(output_ptr + out_idx, acc.to(tl.float16))
```

## PERFORMANCE NOTES
- Reduction over last dimension: Most efficient (coalesced access)
- Reduction over first dimension: Requires strided access (slower)
- For rows <8192: single-pass algorithm
- For rows >8192: multi-pass algorithm
- Expected speedup: 1.5-2.5x for large tensors
- tl.sum/tl.max/tl.min use efficient warp-level reduction

